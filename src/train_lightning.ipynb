{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "DATA_PATH = '../../datasets/crack-detection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<module 'torch.version' from 'D:\\\\conda\\\\envs\\\\pytorch\\\\lib\\\\site-packages\\\\torch\\\\version.py'>\nCUDA 10.2\ncuDNN 7604\n"
    }
   ],
   "source": [
    "print(torch.version)\n",
    "print(f'CUDA {torch.version.cuda}')\n",
    "print(f'cuDNN {torch.backends.cudnn.version()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "There are 20000 negative and 20000 positive samples.\n"
    }
   ],
   "source": [
    "negative_count = len(os.listdir(DATA_PATH + '/Negative/'))\n",
    "positive_count = len(os.listdir(DATA_PATH + '/Positive/'))\n",
    "\n",
    "print(f'There are {negative_count} negative and {positive_count} positive samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrackDetectionModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        model = models.vgg16(pretrained=True)\n",
    "        \n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        num_classes = 2\n",
    "        model.classifier[6] = nn.Linear(4096, num_classes)\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def prepare_data(self):\n",
    "        image_folder = datasets.ImageFolder(DATA_PATH)\n",
    "\n",
    "        train_size = int(0.5 * len(image_folder))\n",
    "        valid_size = int(0.3 * len(image_folder))\n",
    "        test_size  = len(image_folder) - (train_size + valid_size)\n",
    "        train_ds, val_ds, test_ds = torch.utils.data.random_split(image_folder, [train_size, valid_size, test_size])\n",
    "        \n",
    "        print(f'There are {len(train_ds)} train., {len(val_ds)} valid. and {len(test_ds)} test samples.')\n",
    "\n",
    "        train_ds.dataset.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        val_ds.dataset.transform =  transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        test_ds.dataset.transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        self.train_dataset = train_ds\n",
    "        self.val_dataset   = val_ds\n",
    "        self.test_dataset  = test_ds\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        torch.utils.data.DataLoader(self.train_dataset, batch_size=32)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        torch.utils.data.DataLoader(self.val_dataset, batch_size=32)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        torch.utils.data.DataLoader(self.test_dataset, batch_size=32)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "        return optimizer, scheduler\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        y_hat = self.forward(x)\n",
    "        return {'loss': F.cross_entropy(y_hat, y)}\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        y_hat = self.forward(x)\n",
    "        return {'val_loss': F.cross_entropy(y_hat, y)}\n",
    "\n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        x, y = test_batch\n",
    "        y_hat = self.forward(x)\n",
    "        return {'test_loss': F.cross_entropy(y_hat, y)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "GPU available: True, used: False\nNo environment variable for node rank defined. Set as 0.\n\n   | Name               | Type              | Params\n-----------------------------------------------------\n0  | model              | VGG               | 134 M \n1  | model.features     | Sequential        | 14 M  \n2  | model.features.0   | Conv2d            | 1 K   \n3  | model.features.1   | ReLU              | 0     \n4  | model.features.2   | Conv2d            | 36 K  \n5  | model.features.3   | ReLU              | 0     \n6  | model.features.4   | MaxPool2d         | 0     \n7  | model.features.5   | Conv2d            | 73 K  \n8  | model.features.6   | ReLU              | 0     \n9  | model.features.7   | Conv2d            | 147 K \n10 | model.features.8   | ReLU              | 0     \n11 | model.features.9   | MaxPool2d         | 0     \n12 | model.features.10  | Conv2d            | 295 K \n13 | model.features.11  | ReLU              | 0     \n14 | model.features.12  | Conv2d            | 590 K \n15 | model.features.13  | ReLU              | 0     \n16 | model.features.14  | Conv2d            | 590 K \n17 | model.features.15  | ReLU              | 0     \n18 | model.features.16  | MaxPool2d         | 0     \n19 | model.features.17  | Conv2d            | 1 M   \n20 | model.features.18  | ReLU              | 0     \n21 | model.features.19  | Conv2d            | 2 M   \n22 | model.features.20  | ReLU              | 0     \n23 | model.features.21  | Conv2d            | 2 M   \n24 | model.features.22  | ReLU              | 0     \n25 | model.features.23  | MaxPool2d         | 0     \n26 | model.features.24  | Conv2d            | 2 M   \n27 | model.features.25  | ReLU              | 0     \n28 | model.features.26  | Conv2d            | 2 M   \n29 | model.features.27  | ReLU              | 0     \n30 | model.features.28  | Conv2d            | 2 M   \n31 | model.features.29  | ReLU              | 0     \n32 | model.features.30  | MaxPool2d         | 0     \n33 | model.avgpool      | AdaptiveAvgPool2d | 0     \n34 | model.classifier   | Sequential        | 119 M \n35 | model.classifier.0 | Linear            | 102 M \n36 | model.classifier.1 | ReLU              | 0     \n37 | model.classifier.2 | Dropout           | 0     \n38 | model.classifier.3 | Linear            | 16 M  \n39 | model.classifier.4 | ReLU              | 0     \n40 | model.classifier.5 | Dropout           | 0     \n41 | model.classifier.6 | Linear            | 8 K   \nThere are 20000 train., 12000 valid. and 8000 test samples.\nEpoch 1: : 0it [00:00, ?it/s]"
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-370fb62edaf2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\conda\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, train_dataloader, val_dataloaders)\u001b[0m\n\u001b[0;32m    885\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_schedulers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer_frequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_optimizers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 887\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_pretrain_routine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    888\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[1;31m# return 1 when finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mrun_pretrain_routine\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;31m# CORE TRAINING LOOP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1015\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1016\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m     def test(\n",
      "\u001b[1;32mD:\\conda\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    345\u001b[0m                 \u001b[1;31m# RUN TNG EPOCH\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m                 \u001b[1;31m# -----------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m                 \u001b[1;31m# update LR schedulers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[1;31m# run epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m         for batch_idx, (batch, is_last_batch) in self.profiler.profile_iterable(\n\u001b[1;32m--> 406\u001b[1;33m             \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_with_is_last\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"get_train_batch\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m         ):\n\u001b[0;32m    408\u001b[0m             \u001b[1;31m# stop epoch if we limited the number of training batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\profiler\\profilers.py\u001b[0m in \u001b[0;36mprofile_iterable\u001b[1;34m(self, iterable, action_name)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\u001b[0m in \u001b[0;36m_with_is_last\u001b[1;34m(iterable)\u001b[0m\n\u001b[0;32m    798\u001b[0m     \"\"\"Pass through values from the given iterable with an added boolean indicating if this is the last item.\n\u001b[0;32m    799\u001b[0m     See `https://stackoverflow.com/a/1630350 <https://stackoverflow.com/a/1630350>`_\"\"\"\n\u001b[1;32m--> 800\u001b[1;33m     \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    801\u001b[0m     \u001b[0mlast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "model = CrackDetectionModel()\n",
    "\n",
    "trainer = pl.Trainer()\n",
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitpytorchconda3c57f4ade3794c1db54804b75b197582",
   "display_name": "Python 3.6.10 64-bit ('pytorch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}